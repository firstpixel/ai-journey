# ai-journey

# My AI Projects & Learning Journey

A comprehensive log of my AI projects, study materials, and reflections. This document is organized by topic and includes links to GitHub repositories, YouTube videos, articles, and more.

## Table of Contents
- [Projects](#projects)
  - [Neural Networks](#neural-networks)
 <!--  - [Large Language Models (LLMs)](#large-language-models-llms) -->
  - [Text to Speech Models (TTSs)](#text-to-speech-models-ttss)
  - [Agents](#agents)
  - [Other Projects](#other-projects)
- [Learning Resources](#learning-resources)
  - [Courses & Tutorials](#courses--tutorials)
  - [YouTube Videos](#youtube-videos)
  - [Books & Articles](#books--articles)
  - [Papers](#papers)
- [Notes & Reflections](#notes--reflections)
- [Tools & Libraries](#tools--libraries)
- [Terms and Dictionary](#terms-and-dictionary)

---

## Projects

### Neural Networks
- **[Neural network C](https://github.com/firstpixel/c-neural-network/tree/main)**  
  *This project implements a simple neural network in C that learns basic logic functions (XOR, XNOR, OR, AND, NOR, NAND). It uses a single hidden layer with backpropagation and momentum-based updates, along with Xavier initialization for the weights. The network also supports checkpointing so that training can be resumed from a saved state.*
- **[Neural network Python](https://github.com/firstpixel/neural-network)**  
  *This project is a Python port of a C feed-forward neural network featuring one hidden layer, backpropagation with momentum, Xavier weight initialization, checkpointing for saving/loading parameters, and custom logic for logical operations (XOR, XNOR, OR, AND, NOR, NAND).*
<!-- 
### Large Language Models (LLMs)
- **[LLM Project Name](https://github.com/yourusername/llm-project)**  
  *Project overview and key takeaways.*
 -->
### Text to Speech Models (TTSs)
- **[F5-TTS-pt-br](https://huggingface.co/firstpixel/F5-TTS-pt-br)**  
  *Project to learn how to train a TTS Model. Contains pre-trained weights for Portuguese BR in F5-TTS. It only speaks portuguese as it is a preliminary test.

### Agents
- **[Agent Project Name](https://github.com/yourusername/agent-project)**  
  *Short description, technologies used, etc.*

### Other Projects
- **[Other AI Project Name](https://github.com/yourusername/other-ai-project)**  
  *Description...*

---

## Learning Resources

### Courses & Tutorials
- **Neural Networks:**  
  - [Course Name](https://linktocourse.com) - *Brief note on why it's useful or what you learned.*
- **LLMs:**  
  - [Tutorial Name](https://linktotutorial.com) - *A summary of the content and key points.*
- **Agents:**  
  - [Course Name](https://linktocourse.com) - *Summary and insights.*

### YouTube Videos
- **[Video Title](https://youtu.be/yourvideoID)**  
  *A quick summary of the video and its major insights.*
- **[Another Video Title](https://youtu.be/yourvideoID)**  
  *Key takeaways and why it was helpful.*

### Books & Articles
- **[Book/Article Title](https://linktobook.com)**  
  *Summary or notes on the content.*
- **[Another Book/Article Title](https://linktoarticle.com)**  
  *Important concepts and reflections.*

### Papers
- **[Paper Title](https://linktopaper.com)**  
  *Authors, publication venue, and a brief summary of the findings or key contributions.*
- **[Another Paper Title](https://linktopaper.com)**  
  *Authors, key takeaways, and why it's relevant to your studies.*
  
*Feel free to add as many papers as needed, along with notes on the methodologies, experiments, and implications discussed in each.*

---

## Notes & Reflections
- **General Observations:**  
  *Thoughts on trends in AI, challenges faced, and successes achieved.*
- **Project-Specific Learnings:**  
  *What you learned from each project, ideas for improvements, and potential next steps.*

---

## Tools & Libraries
- **Frameworks:** TensorFlow, PyTorch, etc.
- **Libraries:** Hugging Face Transformers, scikit-learn, etc.
- **Other Resources:** Links to documentation, code snippets, etc.
- **[LAPACK](https://www.netlib.org/lapack/lapack-3.12.1.html)**
*LAPACK builds on the BLAS routines to provide higher-level routines for more complex linear algebra tasks.*
- **[BLAS](https://www.netlib.org/blas/)**
*The BLAS (Basic Linear Algebra Subprograms) are routines that provide standard building blocks for performing basic vector and matrix operations.*

---

## Terms and Dictionary

- **Activation Function:**  
  A mathematical function (e.g., sigmoid, ReLU, tanh) applied to a neuron's output to introduce non-linearity.

- **Adversarial Examples:**  
  Inputs intentionally designed to deceive or confuse machine learning models.

- **Agents:**  
  Autonomous entities or programs that perform tasks, make decisions, or interact with environments independently.

- **Agent Workflow:**  
  A defined sequence or process in which agents interact, coordinate, and pass information to complete complex tasks.

- **Artificial Intelligence (AI):**  
  The field focused on creating systems capable of tasks that typically require human intelligence.

- **Backpropagation:**  
  The method used to compute gradients of the loss function with respect to model parameters, enabling effective training.

- **Batch:**  
  A subset of the training data processed in one iteration of the training loop.

- **Batch Normalization:**  
  A technique to standardize the inputs of each layer, stabilizing and accelerating training.

- **BERT (Bidirectional Encoder Representations from Transformers):**  
  A transformer-based model designed for understanding the context of words in a sentence by processing text in both directions.

- **Bias-Variance Tradeoff:**  
  The balance between a model’s ability to minimize bias (error from incorrect assumptions) and variance (error from sensitivity to data fluctuations).

- **Chain-of-Thought:**  
  A prompting strategy in LLMs that encourages generating intermediate reasoning steps before producing a final answer.

- **Convolutional Neural Network (CNN):**  
  A neural network architecture that uses convolutional layers, typically for image and grid-like data.

- **Cross-Validation:**  
  A statistical method to evaluate model performance by partitioning the data into multiple training and validation sets.

- **Data Augmentation:**  
  Techniques used to artificially expand the size and diversity of a training dataset.

- **Data Preprocessing:**  
  The process of cleaning and transforming raw data into a suitable format for modeling.

- **Dataset:**  
  A structured collection of data, often labeled, used for training and evaluating machine learning models.

- **Deep Learning:**  
  A branch of ML that uses multi-layered neural networks to learn high-level representations of data.

- **Dimensionality Reduction:**  
  Techniques (e.g., PCA, t-SNE) that reduce the number of input variables in a dataset while preserving essential information.

- **Diffusion Models:**  
  Generative models that learn to reverse a gradual noising process to synthesize new data samples.

- **Dropout:**  
  A regularization technique where random neurons are ignored during training to prevent overfitting.

- **Encoder-Decoder Architecture:**  
  A model design that encodes input data into a latent representation and then decodes it into an output, common in translation and summarization tasks.

- **Ensemble Methods:**  
  Techniques that combine predictions from multiple models (e.g., random forests, boosting) to improve overall performance.

- **Epoch:**  
  One complete pass through the entire training dataset during the learning process.

- **Embedding:**  
  A dense vector representation of discrete entities (like words) that captures their semantic meaning.

- **Explainability:**  
  Techniques and methods that help interpret and understand the decisions made by machine learning models.

- **Feature Extraction:**  
  The process of deriving informative attributes or features from raw data for use in modeling.

- **Federated Learning:**  
  A decentralized approach where multiple devices collaboratively train a model without sharing raw data.

- **Few-Shot Learning:**  
  The ability of a model to generalize from a very small number of examples.

- **Fine-Tuning:**  
  The process of adapting a pre-trained model to a specific task by further training it on task-relevant data.

- **Generative Adversarial Network (GAN):**  
  A framework where two networks (generator and discriminator) compete, leading to the generation of realistic synthetic data.

- **Gated Recurrent Unit (GRU):**  
  A simplified version of LSTM that also addresses long-term dependencies in sequential data.

- **Gradient Descent:**  
  An optimization algorithm that iteratively adjusts model parameters to minimize a loss function.

- **Graph Neural Network (GNN):**  
  A neural network architecture designed to process data structured as graphs.

- **Hyperparameters:**  
  Configuration settings (like learning rate, batch size, number of layers) that control the learning process.

- **Inference:**  
  The process of using a trained model to make predictions on new, unseen data.

- **Large Language Model (LLM):**  
  A type of deep learning model (often based on transformers) pre-trained on massive text corpora to understand and generate human language.

- **Learning Rate:**  
  A hyperparameter that determines the step size at each iteration while moving toward a minimum of the loss function.

- **Light RAG:**  
  A streamlined version of RAG using a faster, less resource-intensive retrieval process for quicker responses.

- **Loss Function:**  
  A metric that quantifies the error between a model’s predictions and the actual data, guiding the training process.

- **Long RAG:**  
  An approach that retrieves a comprehensive set of documents to provide richer context, with higher computational cost.

- **Long Short-Term Memory (LSTM):**  
  A type of RNN that can capture long-term dependencies by mitigating the vanishing gradient problem.

- **Long Term Memory:**  
  Mechanisms to store information over extended periods, enabling retention of knowledge across sessions.

- **Machine Learning (ML):**  
  A subset of AI that uses data-driven algorithms to enable systems to learn from experience and improve performance on tasks.

- **Meta Learning:**  
  Often described as "learning to learn," where a model is trained on a variety of tasks to improve its ability to learn new tasks quickly.

- **Model Distillation:**  
  The process of transferring knowledge from a large, complex model (teacher) to a smaller, more efficient model (student).

- **Multi-Agent Systems:**  
  Systems where multiple agents collaborate or compete to solve complex problems, often with coordinated or decentralized decision-making.

- **Neural Network:**  
  A computational model composed of layers of interconnected nodes (neurons) that process input data to produce outputs.

- **Overfitting:**  
  When a model learns the details and noise in the training data too well, leading to poor generalization on unseen data.

- **Pretraining:**  
  The initial training phase on a large, generic dataset before fine-tuning the model on a specific task.

- **Prompt:**  
  The input text or query provided to an LLM to guide its generation or responses.

- **Recurrent Neural Network (RNN):**  
  A neural network designed for sequential data, where outputs depend on previous inputs.

- **Reinforcement Learning (RL):**  
  A paradigm where an agent learns to make decisions by interacting with an environment and receiving rewards or penalties.

- **Regularization:**  
  Techniques (like dropout or weight decay) used to reduce overfitting by penalizing overly complex models.

- **Sequence-to-Sequence (Seq2Seq):**  
  An architecture where both input and output are sequences, typically used for tasks like language translation.

- **Self-Attention:**  
  A mechanism that allows a model to weigh the importance of different parts of the input data when constructing its representations.

- **Self-Supervised Learning:**  
  A learning paradigm where the model generates its own labels from data, reducing the need for manually annotated data.

- **Short Term Memory:**  
  Temporary storage of recent information to maintain context during a session or conversation.

- **Supervised Learning:**  
  A learning approach where models are trained using labeled datasets to predict outcomes for new data.

- **Token:**  
  The smallest unit of text (e.g., a word or subword) that models process during training and inference.

- **Transformer:**  
  An architecture that uses self-attention mechanisms to process input data, forming the backbone of many state-of-the-art LLMs.

- **Transfer Learning:**  
  Using a pre-trained model as a starting point for a new, related task.

- **Underfitting:**  
  When a model is too simplistic to capture the underlying structure of the data, resulting in poor performance on both training and new data.

- **Unsupervised Learning:**  
  Techniques that identify patterns or structures in data without using predefined labels (e.g., clustering).

- **Xavier weight initialization:**  
  A method for setting the initial weights of a neural network so that the variance of activations remains similar across layers, helping to prevent issues like vanishing or exploding gradients during training.

- **Zero-Shot Learning:**  
  The ability of a model to perform a task without any task-specific training data.


*Last updated: 2025-02-15*
